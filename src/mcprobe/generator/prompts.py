"""LLM prompts for scenario generation."""

from typing import Any

from pydantic import BaseModel, Field

from mcprobe.generator.mcp_client import ToolSchema


class GeneratedScenarioContent(BaseModel):
    """Content generated by LLM for a scenario."""

    name: str = Field(..., description="Descriptive scenario name")
    description: str = Field(..., description="One sentence describing what this test validates")
    persona: str = Field(..., description="Brief user persona description")
    initial_query: str = Field(..., description="User's opening message")
    known_facts: list[str] = Field(
        default_factory=list, description="Facts the user knows and can provide if asked"
    )
    unknown_facts: list[str] = Field(
        default_factory=list, description="Facts the user doesn't know"
    )
    correctness_criteria: list[str] = Field(
        default_factory=list, description="Criteria for a correct response"
    )
    required_tools: list[str] = Field(default_factory=list, description="Tools that should be used")


CATEGORY_DESCRIPTIONS = {
    "happy_path": """Generate a HAPPY PATH scenario where the user:
- Clearly states what they need
- Provides all required information upfront
- Has a straightforward, achievable goal

The user should ask a direct question that can be answered using the tool
with all necessary parameters clearly specified or easily inferred.""",
    "clarification": """Generate a CLARIFICATION scenario where the user:
- Asks an ambiguous or incomplete question
- Has information but doesn't volunteer it initially
- Requires the agent to ask follow-up questions

The initial query should be missing critical information needed to use the tool.
The agent should need to ask clarifying questions before proceeding.""",
    "edge_case": """Generate an EDGE CASE scenario that tests:
- Missing or invalid data handling
- Boundary conditions (empty inputs, max values, special characters)
- Error recovery and graceful failure

The scenario should test how the agent handles unusual or problematic inputs
that might cause tool failures or unexpected behavior.""",
    "adversarial": """Generate an ADVERSARIAL scenario where the user:
- Provides contradictory information
- Changes requirements mid-conversation
- Tests the agent's ability to handle inconsistency

The scenario should start with one set of requirements and then introduce
conflicting information to test how the agent handles contradictions.""",
    "efficiency": """Generate an EFFICIENCY scenario that tests:
- Minimal tool usage to achieve the goal
- Direct path to the answer
- No unnecessary clarification or redundant calls

The user provides all information upfront like a happy path, but the
evaluation focuses on whether the agent solves it optimally.""",
}


def _format_parameters(input_schema: dict[str, Any]) -> str:
    """Format JSON Schema parameters into readable text."""
    if not input_schema:
        return "No parameters"

    properties = input_schema.get("properties", {})
    required = set(input_schema.get("required", []))

    if not properties:
        return "No parameters"

    lines: list[str] = []
    for name, spec in properties.items():
        param_type = spec.get("type", "any")
        description = spec.get("description", "No description")
        req_marker = " (required)" if name in required else " (optional)"
        lines.append(f"- {name} ({param_type}{req_marker}): {description}")

    return "\n".join(lines)


def build_generation_prompt(
    tool: ToolSchema,
    category: str,
) -> str:
    """Build the LLM prompt for generating scenario content.

    Args:
        tool: The tool schema to generate a scenario for
        category: The scenario category (happy_path, clarification, etc.)

    Returns:
        The formatted prompt string
    """
    params_desc = _format_parameters(tool.input_schema)
    category_description = CATEGORY_DESCRIPTIONS.get(category, CATEGORY_DESCRIPTIONS["happy_path"])

    return f"""You are generating a test scenario for an MCP (Model Context Protocol) tool.

## Tool Information
Name: {tool.name}
Description: {tool.description or "No description provided"}

## Parameters
{params_desc}

## Scenario Type: {category.upper().replace("_", " ")}
{category_description}

## Your Task
Generate the following for this {category} scenario:

1. **name**: A descriptive scenario name (e.g., "{tool.name} {category.replace("_", " ").title()}")
2. **description**: One sentence describing what this test validates
3. **persona**: A brief user persona (who is this user, what's their context)
4. **initial_query**: The user's opening message (natural language, realistic)
5. **known_facts**: List of facts the user knows and can provide if asked
6. **unknown_facts**: List of facts the user doesn't know (especially for clarification/edge_case)
7. **correctness_criteria**: List of 2-4 criteria for evaluating a correct response
8. **required_tools**: List of tools that should be used (usually ["{tool.name}"])

Respond with a JSON object containing these fields."""
